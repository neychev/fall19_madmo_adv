Attacking MNIST classifier:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neychev/fall19_madmo_adv/blob/master/week09_Adversarial/adversarial_example.ipynb)

Attacking Random Forest via mimic model:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neychev/fall19_madmo_adv/blob/master/week09_Adversarial/RandomForest_adversarial_attack.ipynb)


Further readings:
* [ru] [Habr post](https://habr.com/ru/post/370541/) about adversarial examples in nature.
* [en] Good [blogpost by Y Combinator](https://blog.ycombinator.com/how-adversarial-attacks-work/) on adversarial examples.
* [en] Andrej Karpathy [blog post on breaking the classifiers on ImageNet](http://karpathy.github.io/2015/03/30/breaking-convnets/).
* [en] Ian Goodfellow [lecture 16 at Stanford CS231n in 2017](https://www.youtube.com/watch?v=CIfsB_EYsVI)