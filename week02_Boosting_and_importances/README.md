week02 practice feature importances:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neychev/fall19_madmo_adv/blob/master/week02_Boosting_and_importances/week02_feature_importance_exercises.ipynb)


__Further readings__:
* [en] Bias-variance tradeoff in more general case: A Unified Bias-Variance Decomposition and its Applications https://homes.cs.washington.edu/~pedrod/papers/mlc00a.pdf
* [ru] Evgeny Sokolov notes about bias-variance decomposition and Random Forest: https://github.com/esokolov/ml-course-hse/blob/master/2018-fall/lecture-notes/lecture08-ensembles.pdf
* [ru] Great blog post about stacking, blending and their modifications: https://dyakonov.org/2017/03/10/cтекинг-stacking-и-блендинг-blending/
* [ru] Alexander Guschin Bachelor's thesis about stacking: http://www.machinelearning.ru/wiki/images/5/56/Guschin2015Stacking.pdf
* [ru] Great ODS blogpost about gradient boosting: https://habr.com/ru/company/ods/blog/327250/
* [en] Same post as 3 but in English: https://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic10_boosting/topic10_gradient_boosting.ipynb
* [en] Great interactive blogpost by Alex Rogozhnikov: http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html
* [en] And great gradient boosted trees playground by Alex Rogozhnikov: http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html
* [en] Shap values repo and explanation: https://github.com/slundberg/shap
* [en] Kaggle tutorial on feature importances: https://www.kaggle.com/learn/machine-learning-explainability